{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code Ready ETL using Pyspark, VS Code, AWS Redshift, and S3",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOI1frJROoM3FCGCr8I+m+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taiwotman/TaiwotmanGoogleColab/blob/main/Code_Ready_ETL_using_Pyspark%2C_VS_Code%2C_AWS_Redshift%2C_and_S3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRPda4FMsHgZ"
      },
      "source": [
        "***Based on the Article:*** [Code Ready ETL using Pyspark, VS Code, AWS Redshift, and S3 https://taiwo-adetiloye.medium.com/](code-ready-an-etl-process-with-pyspark-aws-redshift-and-s3-967dabb088e3)\n",
        "\n",
        "***Authored by:*** Taiwo O. Adetiloye | Website\n",
        "\n",
        "***Date:*** December 17, 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5sfZVEJ1p9Q"
      },
      "source": [
        "####Install pyspark library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAUYiHPR1zeD"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_Coq7JX2Lrd"
      },
      "source": [
        "####Import key pyspark modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK6gvPPSiLPn"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import col\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xPrHqzb1HpB"
      },
      "source": [
        "####Define the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JcxznVl22ZH"
      },
      "source": [
        "JAR_PATH= \"path/to/jarfile\"\n",
        "JDBC_URL='jdbc:redshift://redshift-cluster-1.cbwit8tvpexx.us-east-1.redshift.amazonaws.com:5439/dev_redshift?user=awsuser&password=<BdFI9{sAB[W],t'\n",
        "TEMP_DIR='s3://data_storage/tempdir/'\n",
        "TABLE_NAME='seattle_emergence_calls'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcTV8YRM1gPT"
      },
      "source": [
        "####Get or Create the SparkSession with the spark name and the required configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3dpJCIb4NFj"
      },
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Seattle Emergency Calls\") \\\n",
        "    .config(\"spark.jars\", JAR_PATH)\\\n",
        "    .config('spark.driver.extraClassPath', JAR_PATH) \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKtYFzqS1P-h"
      },
      "source": [
        "####Load data from AWS RedShift using Pyspark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKAg4SSFFgED"
      },
      "source": [
        "df = spark.read.format(\"jdbc\") \\\n",
        "  .option(\"url\",JDBC_URL ) \\\n",
        "  .option(\"Tempdir\", TEMP_DIR) \\\n",
        "  .option(\"dbtable\", TABLE_NAME) \\\n",
        "  .option(\"forward_spark_s3_credentials\", \"true\") \\\n",
        "  .load()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS04K9rJ0nm4"
      },
      "source": [
        "#### Sample sparkSQL\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnhMApRWrVf_"
      },
      "source": [
        "df.printSchema()\n",
        "\n",
        "df.show(10)\n",
        "\n",
        "df.filter(df.incident_number== 'F190058688').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcHsKXvr03rc"
      },
      "source": [
        "####Sample transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YACH8btSziAz"
      },
      "source": [
        "\n",
        "# Cast longitude and latitude from string to double data type\n",
        "df = df.withColumn(\"latitude\", col(\"latitude\").cast(DoubleType())) \\\n",
        "       .withColumn(\"longitude\", col(\"longitude\").cast(DoubleType()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}